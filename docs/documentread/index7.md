# Learning Transferable Visual Models From Natural Language Supervision

在过去几年中，直接从原始文本学习的预训练方法推动了NLP领域的发展。

很多结果表明，相较于高质量的人工标注 NLP 数据集，现代预训练方法在网页规模的文本集合中所能利用的 总体监督信号 更为丰富。

然而，在计算机视觉领域，依然普遍采用在人工标注的数据集上进行预训练的做法。

然后论文提出：直接从 /  网络文本中学习的预训练模型 / 能否在计算机视觉领域 / 取得类似的突破？

# 数据集

首先，作者从互联网上收集了大量的图片-文本数据对，创建了一个包含 4 亿个图像，文本对的新数据集，称为WIT。

并展示了一个从零开始训练的简化版 ConVIRT，称之为 CLIP.

我们发现，CLIP 与 GPT 系列模型类似，在预训练过程中学会了执行广泛的任务，包括 光学字符识别、地理定位、动作识别 等。

通过在 30 多个现有数据集上进行 零样本迁移测试 来评估 CLIP 的性能，结果表明它可以与以往针对特定任务进行监督训练的模型竞争。

# 预训练方法

由于作者创建的数据集非常大，所以作者在选择预训练方法的时候，把训练效率作为一个很重要的指标。

一开始作者选择的方法类似于VirTex（Learning Visual Representations from Textual Annotations），从零开始联合训练一个图像CNN和文本transformer，以预测图像的描述文本，但是这种方法遇到了困难。

一个拥有6300万参数的Transformer语言模型，它的计算量是ResNet-50 图像编码器的两倍，而且相比于橙色的baseline，它的训练效率也要慢3倍

那么就需要考虑其他预训练方法。

作者发现预测每张图片的精确的描述是很困难的，由于与图像共同出现的文本描述 种类繁多。

因此，作者想到了一个更简单方法：仅仅预测哪一段 文本整体 与哪张图像配对，而不是预测文本中的具体词语。这种方法使训练效率提高了4倍。

# 具体实现

模型输入的是文本-图像对，这些文本和图像分别通过 文本编码器 和 图像编码器 输出对应的特征，然后在这些输出的文本特征和图像特征上进行对比学习。

假设输入 n 对文本-图像对，那么所有的文本和图像总共有 n2 种匹配。联合训练图像编码器和文本编码器，使得图像与文本嵌入之间的余弦相似度在真实的 n 个配对中尽可能大，在 n2-n 个错误配对中尽可能小。

# 

这个是CLIP 实现核心的类似 Numpy 的伪代码。由于数据集很大，因此不用担心过拟合问题；

没有加载预训练权重，完全从零开始训练；

仅使用线性投影，将每个编码器的表示映射到多模态嵌入空间。

# 实验

# 零样本 分类

CLIP的主要目的就是做零样本迁移，因此作者想要训练一个模型，在下游任务中不再需要微调。

作者将CLIP模型用于zero-shot分类任务中，CLIP经过预训练，可以预测图像和文本段是否配对。

对于每个数据集，将数据集中所有类的名称作为一个文本段的集合，输入到文本编码器中得到文本特征的嵌入；

将需要预测的图片输入到图像编码器中得到图像特征的嵌入。

最后计算这些嵌入的余弦相似度，相似度最大的类别作为预测结果。

# 模型选择
模型选择上，

图像编码器主要使用了ResNet-50和Vit。

文本编码器使用Transformer。
 
论文训练了一系列模型，最终使用了一个ViT-L。

然后论文与 Visual N-Grams 作了初步比较，CLIP在全部三个数据集上的性能均有大幅提升。

# 提示工程与集成

此外，作者还发现使用prompt engineering 和 ensembling可以有效提高CLIP的zero-shot分类性能。

同时还观察到，通过为每个任务 定制提示文本 ，零样本性能可以得到显著提升。如A photo of a {label}, a type of pet.

# 零样本 CLIP 性能分析

然后，作者在27个数据集上进行了实验，使用ResNet-50作为baseline。

在大多数分类任务中，例如给车、食物等分类问题上CLIP表现的很好。但是在细粒度分类任务或者专业、复杂或抽象的任务上表现不佳。

# zero-shot与few-shot对比

虽然将零样本方法与全监督模型进行比较可以更好地理解 CLIP 的任务学习能力，但将其与少样本方法进行比较则更为直接，因为零样本可以看作是少样本学习的极限情况。

CLIP的zero-shot性能直接与Bit-M差不多，

但是当学习样本很少时，CLIP的少样本性能反而不如零样本。

# 表征学习

作者将预训练好的CLIP模型应用在下游任务，模型主体冻结梯度，只去微调分类头，使用线性分类器的评估方式，比较各个模型的表征学习能力。

可以看到，CLIP 模型的线性探测性能与最先进的计算机视觉模型的比较中，总体得分和计算效率方面都略胜于现有最佳模型

# 鲁棒性

作者 将零样本 CLIP 与现有 ImageNet 模型在自然分布偏移上的性能进行了比较。可以看到，零样本 CLIP 模型对分布偏移的鲁棒性远高于现有 ImageNet 模型

# 与人类表现的比较 Comparison to Human Performance

分类37种猫或狗的品种的实验中，人类表现从零样本的54%提升到仅一个训练样本的76%，而额外一个训练样本的收益很小。这表明人类“知道自己不知道什么”，并能根据单个样本更新他们对最不确定图像的先验知识。

尽管CLIP在零样本中表现良好，但人类从少量样本学习的方式与本文中的少样本方法存在较大差异。

由于CLIP的少样本评估未有效利用先验知识，而人类做到了，作者推测，找到一种将先验知识适当融入少样本学习的方法是改进CLIP算法的重要一步。

# Conclusion

CLIP将NLP的预训练成功应用到视觉领域，通过自然语言提示实现零样本迁移，性能可媲美监督模型，但仍有改进空间。这项研究展现了跨领域的潜力，同时也带来了值得探讨的社会影响。
