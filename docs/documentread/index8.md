# VicTR: Video-conditioned Text Representations for Activity Recognition

# 介绍

预训练的图像-视觉语言模型（VLMs）可以为视频生成合理的视觉嵌入，并同时生成对应的文本嵌入。

然而，通常这些文本嵌入并不依赖于视觉信息——也就是说，它们对所有视频都是相同的。也就是说，同一个文本描述无论配对哪个视频，其嵌入表示都是一样的

这种方式在基于对比相似性对所有视频进行优化时，缺乏在共享的视觉-语言潜在空间中正确对齐的灵活性。

作者提出了一个方法：基于视频的文本表示。它  让文本表示针对每个视频独立进行调整，使文本嵌入在潜在空间中就能有更多的自由度，从而适应不同的场景

如图所示，对于一个文本 Ta :

当它与视频 Vi 配对时，生成一个特定的文本表示 Tai

当它与另一个视频 Vj 配对时，生成另一个表示 Taj

## Overview of  VicTR （VicTR 概览）

这张图展示了 VicTR 的整体框架。

左边是视频输入，通过图像编码器提取视频帧特征，得到视频的 token；

右边是文本输入，这里分成两类：一类是活动类别的文本提示；另一类是辅助的文本提示，比如物体、场景或者环境信息。

这些文本经过文本编码器会得到对应的 text tokens。

接下来，包括视频的和文本的 所有的 token——都会进入一个联合的视频-文本编码器，生成 视频条件化的文本表示 ，图中用颜色变化来表示这个过程，也就是说，同一句文本在不同视频里得到的表示可以不同。

最后，计算视频 token 和这些视频条件化的文本 token 之间的亲和度，用来完成分类。辅助文本提示的作用，就是为模型提供更多语义信息，帮助它更准确地识别活动。

# 方法

## Detailed view of VicTR compared to prior art
这个是 VicTR 与现有技术的对比图。左上角展示了 视觉语言模型 通用框架。

首先分别用两个独立的编码器对文本提示和视频帧进行编码，然后输入到一个Video head，用来建模时间信息，在Video head 中是否使用文本 token 是可选的。

与之前方法不同，VicTR 允许文本和细粒度的视觉信息对比，还和其他文本信息对比，同时对视觉和文本模态进行联合优化。

它的 video head 包含三个核心操作：Token-boosting、Cross-modal attention 【跨模态注意力】和 Affinity (re-)weighting【亲和度（重新）加权】。

## Token-boosting

为每个视频创建一组专用的文本 token。

方法是复制主干文本编码器的输出，将每帧与文本的亲和力加权调整，为每一帧生成对应的文本 token

具体来说，给定 (n + m) 个文本 token，最终在视频头的输入端得到 T × (n + m) 个专用于每个视频的文本 token。

之后将这些增强的文本标记与视觉标记连接起来，为T×(1+n+m)。

## Cross-modal and Temporal attention

1. 第一个是Cross-modal attention layer跨模态注意力层

每个视觉 token 可以关注同一时间步的所有文本 token；每个文本 token 可以关注该时间步的视觉 token 以及其他文本 token。


2. 第二个是Temporal attention layer时间注意力层

这里的视觉和文本 token 共享一组参数，学习视觉模态中的时间线索（即从单帧视觉嵌入到视频级嵌入），并建模文本模态中语义随时间的变化。

## Affinity (re-)weighting

如前所述，在时间推理的背景下，基于图像-VLM嵌入的原始亲和度可能存在噪声。

由于已经用跨模态和时间信息更新了视觉和文本 token，它们现在处于更好的状态来重新计算亲和力。所以计算新的亲和力值，并相应地重新加权文本 token。

1. 首先将视频和文本 token 拆分

2. 然后对文本 token 进行时间池化（temporally-pool），然后在此基础上执行亲和力重新加权

3. 这些经过亲和力（重新）加权的文本 token 与视觉 token 拼接在一起，然后通过 MLP。


## 分类器

经过L个 Transformer 层后，对所有 token 进行时间池化，最终得到一个单一的视频嵌入、n 个动作-文本嵌入和 m 个辅助文本嵌入。

然后 进一步聚合辅助嵌入使得每个 k 个语义类别对应一个嵌入。

最后，基于亲和力计算 logits。

# 实验

论文进行了 小样本迁移 和 零样本迁移。

还有 短时活动识别 和 长时活动识别 实验。效果都很好。

## 消融实验

### Table1
1. VicTR (No Aux. Text)：去掉辅助文本提示，性能稍微下降，说明辅助文本确实有帮助，但作用比较有限。
2. VicTR (w/ CLIP Visual emb.)：把训练过的 video embedding 换成 CLIP 提供的（即简单的帧特征做平均），结果只掉了一点点：
3. VicTR (w/ CLIP Text emb.)：把他们的 视频条件化的文本表示 换成原始的 CLIP text embedding，性能下降很明显。表明了更新文本表示是最关键的，而不是更新视觉嵌入

### Table2
1. 亲和力加权机制确实有效
2. 分开的注意力（divided attention）比联合注意力更稳定
3. 分类器方面，如果只依赖文本或视觉都会下降，而使用亲和力加权的分类器效果最好。

# Conclusion
提出了 VicTR，把图像预训练多模态模型扩展到视频任务，核心是 视频条件化的文本嵌入。

实验中表明了 语言嵌入在时序推理中的重要作用。